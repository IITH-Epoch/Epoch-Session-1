{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "questionAnswering.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "k9mK2H6gwvpQ"
      },
      "outputs": [],
      "source": [
        "# importing libraries\n",
        "import numpy as np\n",
        "import math\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import seaborn as sns\n",
        "try:\n",
        "    from transformers import AutoTokenizer,AutoModelForQuestionAnswering\n",
        "except:\n",
        "    %pip install transformers\n",
        "    from transformers import AutoTokenizer,AutoModelForQuestionAnswering\n",
        "import typing as ty\n",
        "\n",
        "from transformers import AutoModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qO3jVPCw50f",
        "outputId": "5ef9d436-9792-40b6-821d-5162a85208d7"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# setting sdevice to gpu if it is available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    print(torch.cuda.get_device_name())"
      ],
      "metadata": {
        "id": "noZD2vK6w8Zw"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using a fine-tuned tokenizer\n",
        "\n",
        "path = 'csarron/bert-base-uncased-squad-v1' # use this path instead of the drive one\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/dl_project/Epoch_3\")\n",
        "# using a fine-tuned bert model\n",
        "# using a fine-tuned tokenizer\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"/content/drive/MyDrive/dl_project/Epoch_3\")\n",
        "# sending model to device (gpu if available)\n",
        "model.to(device)\n",
        "# switching from training mode to testing\n",
        "model.eval()\n",
        "model.zero_grad()"
      ],
      "metadata": {
        "id": "M75Tbia1w8rQ"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(inputs : torch.tensor, token_type_ids : torch.tensor = None, \\\n",
        "            position_ids : torch.tensor = None, attention_mask : torch.tensor = None, output_hidden_states = False)\\\n",
        "            -> ty.Tuple[torch.tensor, torch.tensor]:\n",
        "    # if(inputs is not None):\n",
        "    #     inputs = inputs.to(torch.long)\n",
        "    # if(token_type_ids is not None):\n",
        "    #     token_type_ids = token_type_ids.to(torch.long)\n",
        "    # if(position_ids is not None):\n",
        "    #     position_ids = position_ids = position_ids.to(torch.long)\n",
        "    # if(attention_mask is not None):\n",
        "    #     attention_mask = attention_mask - attention_mask.to(torch.long)\n",
        "    output = model(inputs, token_type_ids=token_type_ids,position_ids=position_ids, attention_mask=attention_mask,output_hidden_states = output_hidden_states)\n",
        "    return output.start_logits, output.end_logits"
      ],
      "metadata": {
        "id": "1a0wB_urw-6b"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sep = tokenizer.sep_token_id \n",
        "cls = tokenizer.cls_token_id"
      ],
      "metadata": {
        "id": "7IqPG2rmxClE"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def bert_forward_func(inputs)\n",
        "\n",
        "def construct_inputs(question : str, context : str, cls : torch.tensor, sep : torch.tensor) -> ty.Tuple[torch.tensor, int]:\n",
        "    \"\"\"\n",
        "    Returns\n",
        "    -------\n",
        "    input_ids\n",
        "\n",
        "    input_ids are the sequence of tokens generated after appending \n",
        "    [cls] + question + [sep] + context + [sep]\n",
        "\n",
        "    Here, [sep] is the separator token which is used to mark the end of text,\n",
        "    while [cls] token is used to begin the sequence of tokens.\n",
        "    \"\"\"\n",
        "    question_ids = tokenizer.encode(question, add_special_tokens=False)\n",
        "    text_ids = tokenizer.encode(context, add_special_tokens=False)\n",
        "    input_ids = [cls] + question_ids + [sep] + text_ids + [sep]\n",
        "    input_ids = torch.tensor([input_ids],device=device)\n",
        "    sep_ind = len(question_ids)\n",
        "    return input_ids, sep_ind\n",
        "\n",
        "def construct_attention_mask(input_ids : torch.tensor) -> torch.tensor:\n",
        "    \"\"\"\n",
        "    Returns\n",
        "    -------\n",
        "    attention mask\n",
        "\n",
        "    Attention mask is a tensor with all ones representing where to pay attention.\n",
        "    We are not zero padding input ids, so attention mask with all ones works in\n",
        "    our case\n",
        "    \"\"\"\n",
        "    attention_mask = torch.ones_like(input_ids)\n",
        "    return attention_mask\n",
        "\n",
        "def construct_token_type_ids(input_ids : torch.tensor, sep_ind : int = 0):\n",
        "    \"\"\"\n",
        "    Returns\n",
        "    -------\n",
        "    token type ids\n",
        "\n",
        "    token type ids represent the class of the token, here it is 0 for the tokens\n",
        "    belonging to question, while 1 for separating token and context tokens\n",
        "    \"\"\"\n",
        "    len = input_ids.size(1)\n",
        "    token_type_ids = torch.tensor([[0 if i <= sep_ind else 1 for i in range(len)]], device=device)\n",
        "    return token_type_ids\n",
        "\n",
        "\n",
        "def construct_position_ids(input_ids : torch.tensor) -> torch.tensor:\n",
        "    \"\"\"\n",
        "    Returns\n",
        "    -------\n",
        "    position ids\n",
        "\n",
        "    position ids are the index(position) of each input id token\n",
        "    \"\"\"\n",
        "    len = input_ids.size(1) \n",
        "    position_ids = torch.arange(len, dtype=torch.long, device=device)\n",
        "    position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
        "    return position_ids\n",
        "\n",
        "def decipher_output_token(input_ids : torch.tensor, start: torch.tensor, end : torch.tensor) -> str:\n",
        "    \"\"\"\n",
        "    Returns\n",
        "    -------\n",
        "    The answer to the question from text\n",
        "    \"\"\"\n",
        "    indices = input_ids[0].detach().tolist()\n",
        "    all_tokens = tokenizer.convert_ids_to_tokens(indices)\n",
        "    start_token = torch.argmax(start)\n",
        "    end_token = torch.argmax(end)\n",
        "    output = ' '.join(all_tokens[start_token:end_token+1])\n",
        "    return output"
      ],
      "metadata": {
        "id": "X1Gpg2X4xEwz"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "\n",
        "Context: str = ('The Panthers finished the regular season with a 15–1 record,'\n",
        "                'and quarterback Cam Newton was named the NFL Most Valuable'\n",
        "                'Player (MVP). They defeated the Arizona Cardinals 49–15 in the'\n",
        "                'NFC Championship Game and advanced to their second Super Bowl'\n",
        "                'appearance since the franchise was founded in 1995. The'\n",
        "                'Broncos finished the regular season with a 12–4 record, and'\n",
        "                'denied the New England Patriots a chance to defend their title'\n",
        "                'from Super Bowl XLIX by defeating them 20–18 in the AFC'\n",
        "                'Championship Game. They joined the Patriots, Dallas Cowboys,'\n",
        "                'and Pittsburgh Steelers as one of four teams that have made'\n",
        "                'eight appearances in the Super Bowl. The 2020 regular season'\n",
        "                'win/loss ratio for the Michigan Vikings was 656.')\n",
        "\n",
        "Question: str = 'How many appearances have the Broncos made in the super bowl?'\n",
        "\n",
        "ground_truth = 'eight'\n",
        "\n",
        "print(f'Question: {Question}')\n",
        "print(f'Context: {textwrap.fill(Context, 79)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrdYaIvexJou",
        "outputId": "0910322c-803a-49db-ccd1-50b686b336fa"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: How many appearances have the Broncos made in the super bowl?\n",
            "Context: The Panthers finished the regular season with a 15–1 record,and quarterback Cam\n",
            "Newton was named the NFL Most ValuablePlayer (MVP). They defeated the Arizona\n",
            "Cardinals 49–15 in theNFC Championship Game and advanced to their second Super\n",
            "Bowlappearance since the franchise was founded in 1995. TheBroncos finished the\n",
            "regular season with a 12–4 record, anddenied the New England Patriots a chance\n",
            "to defend their titlefrom Super Bowl XLIX by defeating them 20–18 in the\n",
            "AFCChampionship Game. They joined the Patriots, Dallas Cowboys,and Pittsburgh\n",
            "Steelers as one of four teams that have madeeight appearances in the Super\n",
            "Bowl. The 2020 regular seasonwin/loss ratio for the Michigan Vikings was 656.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Question: str = 'What would cause recession in Europe?'\n",
        "Context: str = 'The cut in supply of gas from Russia might cause Recession in Europe'"
      ],
      "metadata": {
        "id": "MNOywE6T3v-X"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Context = 'Alfred is a white cat and likes apples. Bobby is a black dog which loves mangoes.'\n",
        "Question = 'What does Alfred like?'\n",
        "\n",
        "Context = \"The TA liked the mid-term report. Hence, he awarded full marks to the team. All the team members were very happy.\"\n",
        "Question = 'Why did the team get full marks ?'\n",
        "\n",
        "Context = 'Following Russian invasion of Ukraine, more than a dozen countries have sanctioned Russia. One of them is USA'\n",
        "Question = 'Why did countries sanction Russia?'\n",
        "ground_truth = 'russian invasion of ukraine'"
      ],
      "metadata": {
        "id": "iTQbnqx14Jbe"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids, sep_ind = construct_inputs(Question,Context,cls,sep)\n",
        "indices = input_ids[0].detach().tolist()\n",
        "all_tokens = tokenizer.convert_ids_to_tokens(indices)\n",
        "attention_mask = construct_attention_mask(input_ids)\n",
        "token_type_ids = construct_token_type_ids(input_ids,sep_ind)\n",
        "position_ids = construct_position_ids(input_ids)\n",
        "start, end = predict(input_ids, token_type_ids, position_ids, attention_mask)\n",
        "\n",
        "output = model(input_ids, token_type_ids=token_type_ids,\n",
        "              position_ids = position_ids,\n",
        "              attention_mask=attention_mask,\n",
        "              output_hidden_states = True)\n",
        "\n",
        "print('predicted answer : ', decipher_output_token(input_ids, start, end))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8qWwWTr0gRr",
        "outputId": "38d7be06-9063-4209-b3b0-393461274ebe"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predicted answer :  four\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rw9tJFJIIxcE"
      },
      "execution_count": 80,
      "outputs": []
    }
  ]
}